# GM record classifier

Note: Still in development

The GMC returns 6 predictions for each record:
- A quality Score for each component, generally between 0 - 1  
- A f-min value for each component, between 0.1 - 10 Hz

There is no logic restricting the model to the output ranges so at times they might exceed
the "limits" (have only encountered this for the score, not f-min, but that doesn't mean it isn't possible). 

The results should be used as input for a mapping function that returns a binary classification
for each record, i.e. either usable or not\
This allows task specific classification instead of enforcing certain thresholds via the model.\
A simple mapping function `qqqfff_to_binary` can found in the utils source file.

### Setup
Its recommended to create a new virtual environment see https://docs.python.org/3/library/venv.html  

Clone using
```
git clone git@github.com:ucgmsim/gm_classifier.git
``` 

Install using
```shell script
pip install -e ./gm_classifier
```
All requirements should be install automatically

\
PhaseNet dependency:\
Clone using
```shell script
git clone git@github.com:claudio525/PhaseNet.git
```   

Install using
```shell script
pip install -e ./PhaseNet
``` 

### Making predictions

There are two different ways to get predictions:\
A) Run feature extraction separately from the prediction. This is the recommended way 
when the number of records is large.\
B) Perform feature extraction & prediction together. Not implemented currently.   

#### Approach A:

Run feature extraction using the script `extract_features.py`, which has the following 
arguments:
```
usage: extract_features.py [-h] [--output_prefix OUTPUT_PREFIX] [--event_list_ffp EVENT_LIST_FFP]
                           [--record_list_ffp RECORD_LIST_FFP] [--ko_matrices_dir KO_MATRICES_DIR]
                           [--low_memory]
                           output_dir record_dir

positional arguments:
  output_dir            Path to the output directory
  record_dir            Root directory for the records, will search for V1A records recursively from
                        here

optional arguments:
  -h, --help            show this help message and exit
  --output_prefix OUTPUT_PREFIX
                        Prefix for the output files
  --event_list_ffp EVENT_LIST_FFP
                        Path to file that lists all events to use (one per line). Note: in order to be
                        able to use event filtering, the path from the record_dir has to include a
                        folder with the event id as its name. Formats of event ids: just a number or
                        XXXXpYYYYYY (where XXXX is a valid year)
  --record_list_ffp RECORD_LIST_FFP
                        Path to file that lists all records to use (one per line)
  --ko_matrices_dir KO_MATRICES_DIR
                        Path to the directory that contains the Konno matrices. Has to be specified if
                        the --low_memory options is used
  --low_memory          If specified will prioritise low memory usage over performance. Requires
                        --ko_matrices_dir to be specified.
```
Most of the arguments are optional, however it is recommended to generate the konno matrices 
manually first and pass in the directory.
And if the computer has < 16GB of RAM the `low_memory` option should be used.

To generate the konno matrices use script `gen_konno_matrices.py`, which just requires an output
directory argument.

Example call for `extract_features.py`:
```shell script
python extract_features.py --ko_matrices_dir /home/claudy/dev/work/data/gm_classifier/konno_matrices /home/claudy/dev/work/tmp/gmc_record_test/features /home/claudy/dev/work/tmp/gmc_record_test/2003
```
Note I: The feature extraction is quite slow, so if the number of recrods is large it can easily take multiple hours.\
Note II: Its probably a good idea to pipe the output into a log file, in case some records fail. 
This can be done by adding `> log.txt` to the call above, however this will no longer print the output. To get around this
I suggest to add `|& tee log.txt` to the end of the call instead, which will still periodically print stdout to the terminal (and write to the log file).  

---
Once feature extraction is complete prediction can be done using the `predict.py` script:
```
usage: predict.py [-h] [--model_dir MODEL_DIR] input_dir output_ffp

positional arguments:
  input_dir             Input data directory, can either contain a feature csv file for each component
                        as generated by 'extract_features.py' or geonet V1A files
  output_ffp            Path for the output csv file

optional arguments:
  -h, --help            show this help message and exit
  --model_dir MODEL_DIR
                        Path to the base model directory
```
Where the model directory defaults to the model included in the package

Example call:
```shell script
python predict.py /home/claudy/dev/work/tmp/gmc_record_test/features /home/claudy/dev/work/tmp/gmc_record_test/results.csv
```

#### Approach B:
Not yet implemented 

### Other
- If a compatible GPU is available tensorflow will attempt to use it, however 
unless might cause issues if the GPU memory is not large enough, and since prediction
performance is fast anyways I recommend disabling it using:
```shell script
export CUDA_VISIBLE_DEVICES=-1
```
 

   

